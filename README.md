[![pipeline status](https://gitlab.dantalion.nl:4443/vrije-universiteit-vu-/qemu-csd/badges/master/pipeline.svg)](https://gitlab.dantalion.nl:4443/vrije-universiteit-vu-/qemu-csd/commits/master)
[![coverage report](https://gitlab.dantalion.nl:4443/vrije-universiteit-vu-/qemu-csd/badges/master/coverage.svg)](https://gitlab.dantalion.nl:4443/vrije-universiteit-vu-/qemu-csd/commits/master)
# QEMU-CSD



### Project goals

* Perform read / write requests on ZNS SSD in QEMU.
* Setup Github repository and bi-directional mirroring.
* Read paper.

### Directory structure

* qemucsd - project source files
* cmake - small cmake snippets to enable various features
* dependencies - project dependencies
* docs - doxygen generated source code documentation
* documentation - project documentation written in LaTeX
* playground - small toy examples or other experiments
* [python](python/README.md) - python scripts to aid in visualization or measurements
* tests - unit tests and possibly integration tests

### Modules

| Module     | Optional | Task                                               |
|------------|----------|----------------------------------------------------|
| arguments  | Yes      | Parse commandline arguments to relevant components |

#### Dependencies

This project requires quite some dependencies, the
majority will be compiled by the project itself and installed
into the build directory. Anything that is not automatically
compiled and linked is shown below.

* General
    * compiler with c++17 support
    * cmake 3.13 or higher
* Windows specific
    * Visual Studio 2019 community
    * cygwin
* Documentation
    * doxygen
    * LaTeX
* Code Coverage
    * ctest
    * lcov
    * gcov
* Continuous Integration
    * valgrind
* Python scripts
    * python 3.x
    * virtualenv
* QEMU
    * librdmacm
    * libibverbs
    * libibumad

The following dependencies are automatically compiled. Dependencies are preferably
linked statically due to the nature of this project. However, for several dependencies
this is not possible due to various reason. For Boost, it is because the unit test
framework can not be statically linked (easily):

| Dependency                                                         | Version     |
|--------------------------------------------------------------------|-------------|
| [backward](https://github.com/bombela/backward-cpp)                | 1.5         |
| [booost](https://www.boost.org/)                                   | 1.74.0      |
| [libbpf](https://github.com/libbpf/libbpf)                         | 0.3         |
| [qemu](https://www.qemu.org/)                                      | 5.2.0       |

#### Setup

Building tools and dependencies is done by simply executing the following commands
from the root directory.

```shell script
git submodule update --init --recursive
mkdir build
cd build
cmake ..
cmake --build .
cmake .. # this prevents re-compiling dependencies on every next make command
source qemu-csd/activate.sh
# run commands and tools as you please
deactivate
```

Additionally, any python based tools and graphs are generated by execution these
additional commands from the root directory. Ensure the previous environment has
been deactivated.

```shell script
virtualenv -p python3 python
cd python
source bin/activate
pip install -r requirements.txt
```

#### Licensing

Some files are licensed under a variety of different licenses please see
specific source files for licensing details.

#### References

* [uNVME](https://github.com/OpenMPDK/uNVMe)
* [ZNS SSD QEMU patch v11](http://patchwork.ozlabs.org/project/qemu-devel/list/?series=219344)
* [ZNS SSD QEMU patch v2](https://patchwork.kernel.org/project/qemu-devel/cover/20200617213415.22417-1-dmitry.fomichev@wdc.com/)
* [Zoned storage ZNS SSDs introduction](https://zonedstorage.io/introduction/zns/)
* [NVMe ZNS command set 1.0 ratified TP](https://nvmexpress.org/wp-content/uploads/NVM-Express-1.4-Ratified-TPs-1.zip)
* [Getting started with ZNS in QEMU](https://www.snia.org/educational-library/getting-started-nvme-zns-qemu-2020)

#### Ideas

#### Snippets

How to configure ZNS SSD devices with QEMU
```shell
Usage:
      -device nvme-subsys,id=subsys0
      -device nvme,serial=foo,id=nvme0,subsys=subsys0
      -device nvme,serial=bar,id=nvme1,subsys=subsys0
      -device nvme,serial=baz,id=nvme2,subsys=subsys0
      -device nvme-ns,id=ns1,drive=<drv>,nsid=1,subsys=subsys0  # Shared
      -device nvme-ns,id=ns2,drive=<drv>,nsid=2,bus=nvme2
      
nvme options:
  addr=<int32>           - Slot and optional function number, example: 06.0 or 06 (default: -1)
  aer_max_queued=<uint32> -  (default: 64)
  aerl=<uint8>           -  (default: 3)
  cmb_size_mb=<uint32>   -  (default: 0)
  discard_granularity=<size> -  (default: 4294967295)
  drive=<str>            - Node name or ID of a block device to use as a backend
  failover_pair_id=<str>
  logical_block_size=<size> - A power of two between 512 B and 2 MiB (default: 0)
  max_ioqpairs=<uint32>  -  (default: 64)
  mdts=<uint8>           -  (default: 7)
  min_io_size=<size>     -  (default: 0)
  msix_qsize=<uint16>    -  (default: 65)
  multifunction=<bool>   - on/off (default: false)
  num_queues=<uint32>    -  (default: 0)
  opt_io_size=<size>     -  (default: 0)
  physical_block_size=<size> - A power of two between 512 B and 2 MiB (default: 0)
  pmrdev=<link<memory-backend>>
  rombar=<uint32>        -  (default: 1)
  romfile=<str>
  serial=<str>
  share-rw=<bool>        -  (default: false)
  smart_critical_warning=<uint8>
  subsys=<link<nvme-subsys>>
  use-intel-id=<bool>    -  (default: false)
  write-cache=<OnOffAuto> - on/off/auto (default: "auto")
  x-pcie-extcap-init=<bool> - on/off (default: true)
  x-pcie-lnksta-dllla=<bool> - on/off (default: true)
  zoned.append_size_limit=<size> -  (default: 131072)
  
nvme-ns options:
  bootindex=<int32>
  discard_granularity=<size> -  (default: 4294967295)
  drive=<str>            - Node name or ID of a block device to use as a backend
  logical_block_size=<size> - A power of two between 512 B and 2 MiB (default: 0)
  min_io_size=<size>     -  (default: 0)
  nsid=<uint32>          -  (default: 0)
  opt_io_size=<size>     -  (default: 0)
  physical_block_size=<size> - A power of two between 512 B and 2 MiB (default: 0)
  share-rw=<bool>        -  (default: false)
  subsys=<link<nvme-subsys>>
  uuid=<str>             - UUID (aka GUID) or "auto" for random value (default) (default: "auto")
  write-cache=<OnOffAuto> - on/off/auto (default: "auto")
  zoned.cross_read=<bool> -  (default: false)
  zoned.descr_ext_size=<uint32> -  (default: 0)
  zoned.max_active=<uint32> -  (default: 0)
  zoned.max_open=<uint32> -  (default: 0)
  zoned.zone_capacity=<size> -  (default: 0)
  zoned.zone_size=<size> -  (default: 134217728)
  zoned=<bool>           -  (default: false)
```

Launch QEMU old
```shell
qemu-img create -f raw znsssd.img 0
qemu-system-x86_64 -name qemucsd -m 4G -cpu Haswell -smp 2 -hda ./arch-qemucsd.qcow2 \
-net user,hostfwd=tcp::7777-:22,hostfwd=tcp::2222-:2000 -net nic \
-drive file=./znsssd.img,id=mynvme,format=raw,if=none -device nvme,drive=mynvme,serial=deadbeef,\
logical_block_size=4096,physical_block_size=4096,zoned=true,zone_size=128,zone_capacity=128,\
max_open=0,max_active=0,zone_append_size_limit=128,zone_descr_ext_size=64,zone_file=./zone_meta 
```

```shell
qemu-system-x86_64 -name qemucsd -m 4G -cpu Haswell -smp 2 -hda ./arch-qemucsd.qcow2 \
-net user,hostfwd=tcp::7777-:22,hostfwd=tcp::2222-:2000 -net nic \
\
-drive file=./znsssd.img,id=mynvme,format=raw,if=none \
\
-device nvme-subsys,id=subsys0 \
-device nvme,serial=deadbeef,id=nvme0,zoned.append_size_limit=128,subsys=subsys0 \
-device nvme-ns,id=ns1,drive=mynvme,nsid=1,logical_block_size=4096,physical_block_size=4096, \
zoned=true,zoned.zone_size=128,zoned.zone_capacity=128,zoned.max_open=0,zoned.max_active=0, \
subsys=subsys0
```

```shell
qemu-system-x86_64 -name qemucsd -m 4G -cpu Haswell -smp 2 -hda ./arch-qemucsd.qcow2 \
-net user,hostfwd=tcp::7777-:22,hostfwd=tcp::2222-:2000 -net nic \
\
-drive file=./znsssd.img,id=mynvme,format=raw,if=none \
\
-device nvme,serial=deadbeef,id=nvme0,zoned.append_size_limit=128 \
-device nvme-ns,id=zns1,drive=mynvme,nsid=1,logical_block_size=4096,physical_block_size=4096, \
zoned=true,zoned.zone_size=128,zoned.zone_capacity=128,zoned.max_open=0,zoned.max_active=0, \
bus=nvme0
```

```shell
qemu-img create -f raw blank.img 0
qemu-img create -f raw znsssd.img 16777216
qemu-system-x86_64 -name qemucsd -m 4G -cpu Haswell -smp 2 -hda ./arch-qemucsd.qcow2 \
-net user,hostfwd=tcp::7777-:22,hostfwd=tcp::2222-:2000 -net nic \
-drive file=./blank.img,id=myblank,format=raw,if=none \
-drive file=./znsssd.img,id=mynvme,format=raw,if=none \
-device nvme-subsys,id=subsys0 \
-device nvme,serial=deadbeef,id=nvme0,drive=myblank,zoned.append_size_limit=128,subsys=subsys0 \
-device nvme-ns,id=zns1,drive=mynvme,nsid=1,logical_block_size=4096,\
physical_block_size=4096,zoned=true,zoned.zone_size=131072,zoned.zone_capacity=131072,\
zoned.max_open=0,zoned.max_active=0,bus=nvme0
```


```shell
qemu-system-x86_64 -name qemucsd -m 4G -cpu Haswell -smp 2 -hda ./arch-qemucsd.qcow2 \
-net user,hostfwd=tcp::7777-:22,hostfwd=tcp::2222-:2000 -net nic \
-drive file=./blank.img,id=myblank,format=raw,if=none \
-drive file=./znsssd.img,id=mynvme,format=raw,if=none \
-device nvme-subsys,id=subsys0 \
-device nvme,serial=foo,id=nvme0,subsys=subsys0 \
-device nvme,serial=bar,id=nvme1,subsys=subsys0 \
-device nvme,serial=baz,id=nvme2,subsys=subsys0 \
-device nvme-ns,id=ns1,drive=mynvme,nsid=1,subsys=subsys0 \
-device nvme-ns,id=ns2,drive=myblank,nsid=2,bus=nvme2
```



```shell
qemu-img create -f raw znsssd.img 16777216
qemu-system-x86_64 -name qemucsd -m 4G -cpu Haswell -smp 2 -hda ./arch-qemucsd.qcow2 \
-net user,hostfwd=tcp::7777-:22,hostfwd=tcp::2222-:2000 -net nic \
-drive file=./znsssd.img,id=mynvme,format=raw,if=none \
-device nvme-subsys,id=subsys0 \
-device nvme,serial=baz,id=nvme2,zoned.append_size_limit=131072,subsys=subsys0 \
-device nvme-ns,id=ns2,drive=mynvme,nsid=2,logical_block_size=4096,\
physical_block_size=4096,zoned=true,zoned.zone_size=131072,zoned.zone_capacity=131072,\
zoned.max_open=0,zoned.max_active=0,bus=nvme2
```


Demo for friday afternoon
```shell
cat /sys/block/nvme0n1/queue/zoned
cat /sys/block/nvme0n1/queue/chunk_sectors
cat /sys/block/nvme0n1/queue/nr_zones
sudo blkzone report /dev/nvme0n1
sudo nvme zns id-ns /dev/nvme0n1
sudo nvme zns report-zones /dev/nvme0n1
sudo nvme zns open-zone /dev/nvme0n1 -s 0xe40
sudo nvme zns finish-zone /dev/nvme0n1 -s 0xe40
sudo nvme zns report-zones /dev/nvme0n1
sudo nvme zns reset-zone /dev/nvme0n1 -s 0xe40
```