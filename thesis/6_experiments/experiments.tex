% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Experiments} % top level followed by section, subsection


% ----------------------- contents from here ------------------------
% 

\section{Experiment Design}

In this section we describe several experiments to evaluate the performance of
FluffleFS. These experiments can be separated into two categories. Firstly,
microbenchmarks operating on relatively simple reads and writes.
Secondly, isolated applications that likely benefit from offloading
ported to be used by FluffleFS. For each of these applications we reason
about the expected and achieved data movement reduction among other performance
metrics. Both microbenchmarks and applications are evaluated using iodepth one
meaning a maximum of one I/O request is pending before the next one is
submitted.

Additionally, each of the experiments is measured 30 times to account for
variance between measurements. For almost all measurements the mean will be used
alongside minimum and maximum errors bars.

\subsection{Microbenchmarks}

Most of these microbenchmarks serve to compare performance against more
established ZNS supporting filesystems. For these microbenchmarks the popular
F2FS \cite{Lee2015F2FSAN} filesystem is used. Exceptions to these include
evaluating the offloading and concurrent performance of FluffleFS against its
regular performance.

Each of these benchmarks is executed on a freshly formatted filesystem each
time due to the inability of FluffleFS to reach \textit{Steady State}. while
F2FS is able to reach steady state a comparison between filesystems in such
substantially different states would be unfair.

Briefly, the following four microbenchmarks will be evaluated:

\begin{enumerate}
    \item Sequential reads \& writes F2FS vs FluffleFS
    % (Linegraph transfer rate MB/s - 64k up until 1g filesizes)
    % (Static request size of 512k unless file is smaller)
    % \begin{itemize}
    %     \item Demonstrate what performance can be expected from FluffleFS as
    %           regular filesystem.
    % \end{itemize}
    \item Random reads \& writes F2FS vs FluffleFS
    % (Linegraph transfer rate MB/s - 4k, 8k, 16k, 32k, 64k, 128k request sizes)
    % (Static file size of 1g, this is to exhaust caches and their effects)
    % \begin{itemize}
    %     \item Demonstrate what performance can be expected from FluffleFS as
    %           regular filesystem.
    % \end{itemize}
    \item Sequential reads \& writes regular vs passthrough kernel
    % (Linegraph transfer rate MB/s - 64k up until 1g filesizes)
    % (Static request size of 512k unless file is smaller)
    % \begin{itemize}
    %     \item Show performance impact of snapshotting and uBPF.
    % \end{itemize}
    \item Sequential reads \& writes concurrently regular \& 1 passthrough 512k
    request size
    % (Linegraph, 1, 2, 4, 8 threads, with best performer previous benchmark)
    % \begin{itemize}
    %     \item Demonstrate the passthrough kernel performance is relatively
    %           unaffected compared to deminishing performace of regular reads
    %           \& writes for multiple threads. (Proof concurrent access to the
    %           same file).
    % \end{itemize}
\end{enumerate}

These initial two comparisons between F2FS and FluffleFS establish a baseline of
the performance that can be expected from the current prototype. However, this
current prototype is not designed to beat the performance of well established
flash optimized filesystems. Instead such a baseline can be used for additional
reasoning in other microbenchmarks or application evaluations. Subsequent
microbenchmarks evaluate the performance overhead of the offloading through
uBPF without and with concurrent regular writes respectively.

Each of these microbenchmarks is used to generate a linegraph measuring
throughput in MiB/S\footnotemark[16] as performance metric. For sequential
benchmarks the evaluaton is performed with varying file size sizes while for
random benchmarks request size (stride) is varied. Contrarily, sequential
benchmarks use a fixed request size of 512 KiB while random benchmarks use a
fixed file size of 1 GiB.

\footnotetext[16]{MebiBytes (MiB) refers to 1048576 bytes where MegaBytes (MB)
would refer to 1000000 bytes in total. These distinctions are denoted by the
lowercase i in the acronym. Every denotion with such a lowercase i is a power
of two where without they refer to clean multiples of ten. This distinction is
used to prevent ambiquity that was historically problematic in computer science
literature.}

Combined these microbenchmarks establish the baseline performance of the current
prototype filesystem as well as demonstrate the performance impact of
snapshotted offloading. Finally, we establish the performance impact on
offloading when concurrent writes are going to the same file.

\subsection{Applications}

The evaluation of applications focusses on proving the practicality of a 
filesystem supporting CSx. To achieve this our application is designed to show
that both host CPU load can be decreased as well as the amount of data that
needs to be moved from the drive to the host. For this a common algorithm to
compute \textit{shannon entropy} (entropy in information theory) \cite{6773024}
is ported to be computed by a kernel.

Typically, shannon entropy is used by compression algorithms such as those used
for transparent compression in filesystems. The algorithm works by computing
the distribution of outcomes from a set of possible outcomes. Most if not all
compression algorithms will use one byte, or 256 possibilities as the set of
possible outcomes. If the change for each outcome is equal to all other
outcomes the result will be one\footnotemark[17]. Meaning, no compression can be
achieved. Contrarily, should all outcomes be the same outcome the result will
be zero. Typically, compression algorithms only compute shannon entropy for a
portion of the file and utilize a cut-off value to decide to compress the file
or not. It should be noted that the actual theory behind shannon entropy is more
involved and much more broadly applicable. Instead, this description is
purposely written to demonstrate how it can be used to determine
\textit{compressability}.

\footnotetext[17]{Our specific implementation will return eight in this case
instead of one as the output is scaled according to the number of bits used to
generate the set of possible outcomes.}

This task can easily be performed in the background potentially storing the file
first in uncompressed state and compressing it later. Such background
applications have loose timing constraints making them excellent use cases for
computational storage. Reasoning is that compute capability on a CSx will
typically be significantly less compared to the host processor. Applications
need to take the response time end users expect as well as the amount of
computation required into account.

% \begin{enumerate}
    % \item Offload integer average (read stream kernel)
    % \begin{itemize}
    %     \item Have one thread write append to a file while the another computes
    %           the integer average for this file do so using both regular access
    %           and using kernel offloading.
    %     \item Demonstrate offloading can reduce amount of data transfered
    %           between host and device significantly with no signifcant
    %           performance impact.
    % \end{itemize}
    % \item Offloaded shannon entropy (read stream kernel)
    % \begin{itemize}
        % \item Go through all files in a directory and determine entropy of first
        %       512k of each file. Return a list of files with entropy < 6.
        % \item Demonstrate that async applications can reduce host load by
        %       waiting for offloaded tasks (Total CPU time - kernel CPU time)
    % \end{itemize}
    % \item CSV index generation (write event kernel)
    % \begin{itemize}
    %     \item Generate indexes for a query replicated from TPC-DS at 512k
    %           intervals.
    %     \item Demonstrate that async applications can reduce host load by
    %           waiting for offloaded tasks (Total CPU time - kernel CPU time)
    % \end{itemize}
% \end{enumerate}

\section{Experimental Setup}

This section briefly describes the physical machine used during evaluation.
In addition, the parameters used with the popular virtual machine called QEMU
\cite{qemu} are detailed. Lastly, an overview of software dependencies and their
respective version as well as compiler flags is given.

Firstly, the system uses a Intel Xeon Silver 4210 processor running at 2.20GHz.
However, due to hardware vulnerabilities in these series of Intel processors
hyperthreading has been explicitly disabled. From the total pool of physical
memory 32GiB has been allocated to the QEMU virtual machine. Other QEMU
allocations include PCIe passthrough for a ZNS SSD, model number
WZS4C8T4TDSP303, courtesy of Western Digital. This ZNS device has a total
capacity of 4TB (Not 4TiB) of which about 4GB can be configured as conventional
block storage. This conventional block storage is beneficial in combination with
F2FS that requires it when used with a ZNS device.

Other QEMU configuration parameters include the use of
\textit{Kernel-based Virtual Machine} (KVM) \cite{kvm} and eight out of the
total CPU cores on the Xeon 4210. For completeness the entire set of parameters
is shown in figure \ref{figure:qemuparameters}.

\begin{figure}
    \centering
	\bashexternal{resources/bash/qemu-parameters.sh}
	\caption{Complete set of parameters as passed to QEMU during experiments}
    \label{figure:qemuparameters}
\end{figure}

Apart from QEMU the prototype has many other software dependencies such as SPDK.
However, QEMU and SPDK are dependencies through different mechanisms. These
dependencies can be divided into two categories being support and operation
dependencies respectively. Support dependencies are used to perform compilation
or provide the environemnt for the prototype to run in. Distinctly, they are not
linked to any binaries of the prototype at compile time. Contrarily, operation
dependencies are linked at compile time. An overview of all support dependencies
is shown in table \ref{table:supportdependencies}.

\begin{table}
    \caption{Overview of support dependencies and their respective versions}
    \centering
    \begin{adjustbox}{width=1\textwidth}
        \begin{threeparttable}[]
            \begin{tabular}{lllll}
                \toprule
                \textbf{Name} & \textbf{Role} & \textbf{Version} & \textbf{Purpose} \\
                \midrule
                Linux & Environment & 5.15 - 5.18 & Underlying OS \\
                QEMU & Environment & 6.1.0 & Virtualization \\
                Clang & Compilation & 10 - 14 & Compiling eBPF bytecode \\
                GCC & Compilation & 12 & Compiling framework \& filesystem \\
                CMake & Compilation & 3.18 - 3.23 & Orchestrate compile targets and linking \\
                Meson & Compilation & 0.59 & Operation dependency build tool \\
                pyelftools & Compilation & 0.28 & Operation dependency build tool \\
                ninja & Compilation & 1.11 & Operation dependency build tool \\
                cunit & Testing & 2.1.3 & Unit testing \\
                ctest & Testing & 2.1.3 & Unit testing \\
                lcov & Testing & 1.16 & Code coverage reports \\
                gcov & Testing & 5.2 & Code coverage reports \\
                valgrind & Testing & 3.19.0 & Leak testing \\
                BASH & Automation & 5.1.16 & Experimental analysis \& result extraction \\
                Python & Automation \& Graphing & 3.9 - 3.10 & Experimental analysis \& result graphs \\
                Virtualenv & Environment & 20.11 & Python dependency isolation \\
                \bottomrule
            \end{tabular}
        \end{threeparttable}
        \label{table:supportdependencies}
    \end{adjustbox}
\end{table}


A similar table for operation dependencies is shown in table
\ref{table:operationdepdencies}.

\begin{table}
    \caption{Overview of operation dependencies and their respective versions}
    \centering
    \begin{adjustbox}{width=1\textwidth}
        \begin{threeparttable}[]
            \begin{tabular}{lllll}
                \toprule
                \textbf{Name} & \textbf{Role} & \textbf{Version} & \textbf{Purpose} \\
                \midrule
                backward & Debugging & 1.6 & Stack traces \\
                Boost & Parsing \& Testing & 1.74.0 & Argument parsing \& unit tests \\
                libfuse & Core component & 3.10.5 & Userspace filesystem interface \\
                SPDK & Core component & 22.01 & Userspace NVMe driver \\
                DPDK & Core component & spdk-22.11 & SPDK backend \\
                ISA-L & Core component & spdk-v2.30.0 & SPDK dependency \\
                uBPF & Core component & Commit \href{https://github.com/iovisor/ubpf/commit/9eb26b4bfdec6cafbf629a056155363f12cec972}{9eb26b4} & eBPF VM \\
                xenium & Instrumentation & Commit \href{https://github.com/mpoeter/xenium/commit/f1d28d0980cf2128c3f6b77d321aad5ca469dbce}{f1d28d0} & Performance measurements \\
                \bottomrule
            \end{tabular}
            % \begin{tablenotes}[para,flushleft]
            %     \centering Overview of CSx works and the previously elaborated
            %     characteristics.
            % \end{tablenotes}
        \end{threeparttable}
        \label{table:operationdepdencies}
    \end{adjustbox}
\end{table}

This thorough description of experimental setup is aimed to simplify
repeatability of our work. In addition, it is important to re-emphasize that the
entire work is open-source and readily available online \cite{qemu-csd}. This
includes scripts to perform the evaluations, raw measurements from these
evaluations, the python code used to generate graphs and even all LaTeX source
code to documents such as this thesis. In the field of computer science
repeatability and verifiability are often overlooked. We hope this works sets a
precident for the degree of openness that should be expected in our field.

% \subsection{Pitfalls}

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------